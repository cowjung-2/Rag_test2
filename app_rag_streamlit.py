# C:\streamlit\app_rag_streamlit.py
import os, re, time
import streamlit as st
import tiktoken
from loguru import logger

# --- LangChain 0.2.x ê³„ì—´ ---
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader
try:
    # í´ë°±: ì¶”ì¶œë¥  ë†’ì€ ë¡œë”(ì„¤ì¹˜ í•„ìš”: pymupdf)
    from langchain_community.document_loaders import PyMuPDFLoader
except Exception:
    PyMuPDFLoader = None

try:
    from langchain_community.document_loaders import Docx2txtLoader
except Exception:
    Docx2txtLoader = None
try:
    from langchain_community.document_loaders import UnstructuredPowerPointLoader
except Exception:
    UnstructuredPowerPointLoader = None

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except Exception:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain_google_genai import ChatGoogleGenerativeAI
from google.api_core.exceptions import ResourceExhausted

import google.generativeai as genai

# =========================
# Utils
# =========================
def tiktoken_len(text: str) -> int:
    tok = tiktoken.get_encoding("cl100k_base")
    return len(tok.encode(text))

def _chars(docs):
    return sum(len(d.page_content or "") for d in docs)

def _preview(text, n=300):
    text = (text or "").strip().replace("\n", " ")
    return text[:n] + ("..." if len(text) > n else "")

def load_pdf_any(path):
    """PDF í…ìŠ¤íŠ¸ ë¡œë”©: PyPDF â†’ (ë¹„ì–´ìˆìœ¼ë©´) PyMuPDF í´ë°±"""
    docs = []
    try:
        docs = PyPDFLoader(path).load_and_split()
    except Exception as e:
        logger.warning(f"PyPDFLoader ì‹¤íŒ¨: {e}")

    if _chars(docs) < 50 and PyMuPDFLoader is not None:
        try:
            docs = PyMuPDFLoader(path).load_and_split()
            logger.info("PyMuPDFLoader í´ë°± ì‚¬ìš©")
        except Exception as e:
            logger.warning(f"PyMuPDFLoader ì‹¤íŒ¨: {e}")
    return docs

def load_docs(files):
    """ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ í›„ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¡œë“œ(+PDF í´ë°±)"""
    docs = []
    for f in files:
        name = f.name
        with open(name, "wb") as o:
            o.write(f.getvalue())
        lower = name.lower()
        cur = []
        if lower.endswith(".pdf"):
            cur = load_pdf_any(name)
        elif lower.endswith(".docx") and Docx2txtLoader:
            cur = Docx2txtLoader(name).load_and_split()
        elif lower.endswith(".pptx") and UnstructuredPowerPointLoader:
            cur = UnstructuredPowerPointLoader(name).load_and_split()
        else:
            logger.warning(f"Unsupported or missing dependency for: {name}")
            continue

        docs.extend(cur)
        # ì‚¬ì´ë“œë°”ì— per-file í†µê³„ ì¶œë ¥
        st.sidebar.write(f"ğŸ“„ **{name}** â†’ pages: {len(cur)}, chars: {_chars(cur)}")
        if lower.endswith(".pdf") and _chars(cur) < 50:
            st.sidebar.warning("ì´ PDFëŠ” í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº”ë³¸(OCR í•„ìš”) ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.")
    return docs

def split_docs(documents):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=120, length_function=tiktoken_len
    )
    return splitter.split_documents(documents)

def build_vector(chunks):
    emb = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )
    return FAISS.from_documents(chunks, emb)

# =========================
# ëª¨ë¸ íƒìƒ‰/ì„ íƒ (+429 í´ë°±)
# =========================
def discover_models():
    genai.configure(api_key=os.environ.get("GOOGLE_API_KEY", ""))
    names = []
    for m in genai.list_models():
        methods = getattr(m, "supported_generation_methods", []) or getattr(m, "generation_methods", [])
        if "generateContent" in methods:
            names.append(m.name.split("/")[-1])
    return names

def pick_model_dynamic():
    avail = discover_models()

    forced_list = [m.strip() for m in os.getenv("GEMINI_MODEL_LIST", "").split(",") if m.strip()]
    if forced_list:
        forced_list = [m for m in forced_list if m in avail]
        if forced_list:
            return forced_list

    # Gemma/Pali/-exp ì œì™¸
    filtered = [m for m in avail if not (m.startswith("gemma") or m.startswith("pali") or "-exp" in m)]

    # 1.5/í”„ë¡œ ìš°ì„ 
    ordered = []
    pref_15 = [r"gemini-1\.5-flash-.*", r"gemini-1\.5-pro-.*", r"gemini-1\.5-flash", r"gemini-1\.5-pro", r"gemini-pro"]
    for pat in pref_15:
        for m in filtered:
            if re.fullmatch(pat, m) and m not in ordered:
                ordered.append(m)

    # ì—†ìœ¼ë©´ 2.x(ì¿¼í„° ì£¼ì˜)
    pref_2x = [r"gemini-2\.0-flash-.*", r"gemini-2\.0-pro-.*", r"gemini-2\.5-flash-.*",
               r"gemini-2\.0-flash", r"gemini-2\.0-pro", r"gemini-2\.5-flash", r"gemini-2\.5-pro"]
    if not ordered:
        for pat in pref_2x:
            for m in filtered:
                if re.fullmatch(pat, m) and m not in ordered:
                    ordered.append(m)

    if not ordered:
        st.sidebar.error(f"ë°œê²¬ëœ ëª¨ë¸(ì°¸ê³ ): {avail}")
        raise RuntimeError("ì´ KEYë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Gemini í…ìŠ¤íŠ¸ ëª¨ë¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return ordered

def make_llm_with_fallback():
    tried = []
    candidates = pick_model_dynamic()
    for model_id in candidates:
        try:
            llm = ChatGoogleGenerativeAI(model=model_id, temperature=0, max_output_tokens=512)
            _ = llm.invoke("ping").content  # ìµœì†Œ í˜¸ì¶œë¡œ ê°€ìš©ì„± í™•ì¸
            return llm, model_id, candidates
        except ResourceExhausted:
            tried.append((model_id, "quota_exhausted")); continue
        except Exception as e:
            tried.append((model_id, f"{type(e).__name__}")); continue
    raise RuntimeError(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ì‹œë„ ë‚´ì—­: {tried}")

def get_conversation_chain(vs):
    llm, picked, cand = make_llm_with_fallback()
    # ì…ë ¥ í† í° ì ˆì•½ + ê²€ìƒ‰ ì •í™•ë„ ë³´ì™„(ìµœì†Œ 2ê°œ ê°€ì ¸ì™€ì„œ MMR)
    retriever = vs.as_retriever(search_type="mmr", search_kwargs={"k": 2, "fetch_k": 6})
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        memory=ConversationBufferMemory(
            memory_key="chat_history", return_messages=True, output_key="answer"
        ),
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=False,
    )
    return chain, picked, cand

# =========================
# ìŠ¤ë¡œí‹€ + ë°±ì˜¤í”„
# =========================
def throttle(min_interval=2.5):
    st.session_state.setdefault("last_ts", 0.0)
    now = time.time()
    if now - st.session_state["last_ts"] < min_interval:
        st.warning(f"ìš”ì²­ ê°„ê²©ì„ {min_interval}ì´ˆ ì´ìƒìœ¼ë¡œ í•´ì£¼ì„¸ìš”.")
        st.stop()
    st.session_state["last_ts"] = now

def backoff_call(fn, tries=3, base=2.0):
    for i in range(tries):
        try:
            return fn()
        except ResourceExhausted:
            wait = base ** i
            st.warning(f"429(ë¦¬ë°‹). {wait:.1f}s ëŒ€ê¸° í›„ ì¬ì‹œë„â€¦")
            time.sleep(wait)
    return fn()

# =========================
# App
# =========================
def main():
    st.set_page_config(page_title="Streamlit_RAG", page_icon="ğŸ“š")

    # secrets â†’ env
    try:
        if "GOOGLE_API_KEY" in st.secrets:
            os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
    except FileNotFoundError:
        pass

    st.title("_Private Data :red[Q/A Chat]_ ğŸ“š")

    st.session_state.setdefault("messages", [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬¸ì„œ ì—…ë¡œë“œ í›„ Processë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”."}])
    st.session_state.setdefault("conversation", None)
    st.session_state.setdefault("vectorstore", None)
    st.session_state.setdefault("model_id", None)
    st.session_state.setdefault("candidates", [])
    st.session_state.setdefault("ready", False)

    with st.sidebar:
        files = st.file_uploader("Upload files", type=["pdf", "docx", "pptx"], accept_multiple_files=True)
        api_key = st.text_input("Google API Key (ì˜µì…˜: secrets.toml ì‚¬ìš© ì‹œ ë¹„ì›Œë‘ê¸°)", type="password")
        if api_key:
            os.environ["GOOGLE_API_KEY"] = api_key
        if st.button("Process"):
            st.session_state["ready"] = False
            st.session_state["conversation"] = None
            st.session_state["vectorstore"] = None
            st.session_state["model_id"] = None
            st.session_state["candidates"] = []

            if not os.environ.get("GOOGLE_API_KEY"):
                st.error("Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤ (secrets.toml ë˜ëŠ” ì…ë ¥)."); st.stop()
            if not files:
                st.error("ìµœì†Œ 1ê°œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."); st.stop()

            raw_docs = load_docs(files)
            total_chars = _chars(raw_docs)
            st.sidebar.write(f"ğŸ” total pages: {len(raw_docs)}, total chars: {total_chars}")
            if total_chars < 50:
                st.sidebar.error("ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê±°ì˜ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìŠ¤ìº”ë³¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤(OCR í•„ìš”).")

            if not raw_docs:
                st.error("ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()

            chunks = split_docs(raw_docs)
            st.sidebar.write(f"chunks: {len(chunks)}")
            if chunks:
                st.sidebar.write("preview:", _preview(chunks[0].page_content))

            vs = build_vector(chunks)
            try:
                chain, model_id, cand = get_conversation_chain(vs)
            except Exception as e:
                st.error("LLM ì´ˆê¸°í™” ì‹¤íŒ¨"); st.exception(e); st.stop()

            st.session_state["vectorstore"] = vs
            st.session_state["conversation"] = chain
            st.session_state["model_id"] = model_id
            st.session_state["candidates"] = cand
            st.session_state["ready"] = True
            st.success(f"ì¤€ë¹„ ì™„ë£Œ! ëª¨ë¸: **{model_id}**")

    with st.sidebar.expander("ğŸ”§ Diagnostics"):
        st.write("GOOGLE_API_KEY:", "âœ…" if os.environ.get("GOOGLE_API_KEY") else "âŒ")
        st.write("Ready:", st.session_state["ready"])
        st.write("Selected model:", st.session_state["model_id"])
        if st.session_state["candidates"]:
            st.write("Candidates:", st.session_state["candidates"])

    # ë©”ì‹œì§€ ë Œë”
    for m in st.session_state["messages"]:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # ì§ˆì˜
    if q := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
        if not st.session_state["ready"]:
            st.warning("ë¨¼ì € íŒŒì¼ ì—…ë¡œë“œ í›„ Processë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”."); st.stop()

        throttle(2.5)
        st.session_state["messages"].append({"role": "user", "content": q})
        with st.chat_message("user"):
            st.markdown(q)

        with st.chat_message("assistant"):
            try:
                result = backoff_call(lambda: st.session_state["conversation"]({"question": q}))
            except Exception as e:
                st.error("í˜¸ì¶œ ì˜¤ë¥˜"); st.exception(e); st.stop()

            answer = result.get("answer", "")
            st.markdown(answer)
            st.session_state["messages"].append({"role": "assistant", "content": answer})

            src = result.get("source_documents") or []
            if src:
                with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                    for i, d in enumerate(src[:3], 1):
                        src_path = d.metadata.get("source", f"doc_{i}")
                        st.markdown(f"- **{src_path}**")
                        st.caption(d.page_content[:500] + ("..." if len(d.page_content) > 500 else ""))

if __name__ == "__main__":
    main()
